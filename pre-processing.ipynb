{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Droughts with Meteorological Data\n",
    "### Pre-processing and then saving data as files to save time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## Loading the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the csv file paths for training, validation and testing into the ``files`` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from loader import *\n",
    "from normalizer import *\n",
    "from viz_report import *\n",
    "\n",
    "files = {'test': 'test_timeseries.csv',\n",
    "        'train': 'train_timeseries.csv'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a helper method to load the datasets. This just walks through the json and discards the few samples that are corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csvs only when we need to create new data\n",
    "def get_dfs(files, file_list):\n",
    "    return {k: pd.read_csv(files[k]).set_index(['fips', 'date'])\n",
    "            for k in file_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = get_dfs(files, ['test', 'train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the day of year using sin/cos and add the data loading function `loadXY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add a helper to normalise the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load our training data set, where X consists of static (soil) and time (meteorological) data and Y consists of the future drought values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d682dd3b0291401787834aa7cee31341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1366099 samples\n",
      "train shape (1366099, 14, 21)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcc942fb4d64ba2a565941ab9efc12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7334a84f9d3f4506afe39fef757a26d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WINDOW_SIZE = 14\n",
    "X_static_train, X_time_train, y_target_train = loadXY(dfs, \"train\",\n",
    "                                                     window_size=WINDOW_SIZE)\n",
    "print(\"train shape\", X_time_train.shape)\n",
    "\n",
    "normer = Normalizer()\n",
    "X_static_train, X_time_train = normer.normalize(X_static_train, X_time_train, fit=True, with_var=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> The previous blocks have (mostly) been copied from the kaggle challenge starter notebook. My own contributions in plotting functions, models, etc. begin here.</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line of above data frame:\n",
      "[ 0.52218561  0.80873286 -0.20999945 -0.42301141 -0.19271136 -0.1922419\n",
      " -0.36496363 -0.43362918  0.42423737 -0.41377866 -0.59771856 -0.65405067\n",
      " -0.13323254 -0.76847603 -0.41170154 -0.56177661 -0.3475337  -0.27081785\n",
      "  2.13890924  0.11372939  0.97378225]\n",
      "...\n",
      "Last line of above data frame:\n",
      "[-0.11189692  0.82182974 -0.85245538 -1.12233853 -1.26041428 -1.24217844\n",
      " -1.05793255 -1.06844306  0.16683492 -1.09781127 -0.51546371 -0.58715912\n",
      " -0.08327034 -0.69660417 -0.38980252 -0.82298484  0.18748529 -1.24678408\n",
      "  2.13890924  0.31874101  0.9212121 ]\n",
      "\n",
      "Labels to be predicted:\n",
      "[2. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('First line of above data frame:')\n",
    "print(X_time_train[0][0])\n",
    "print('...')\n",
    "print('Last line of above data frame:')\n",
    "print(X_time_train[0][-1])\n",
    "print()\n",
    "print('Labels to be predicted:')\n",
    "print(y_target_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create ``X_train``, we are flattening out the time data, pretending there is no temporal component. Then we are concatenating the static soil data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1366099, 14, 21)\n",
      "(1366099, 294)\n",
      "(1366099, 30)\n",
      "(1366099, 324)\n"
     ]
    }
   ],
   "source": [
    "print(X_time_train.shape)\n",
    "X_train = np.array(list(map(lambda x: x.flatten(), X_time_train)))\n",
    "print(X_train.shape)\n",
    "print(X_static_train.shape)\n",
    "X_train = np.concatenate((X_train, X_static_train), axis=1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``round_and_intify()`` rounds interpolated drought values like 1.21 into clean integers between 0 and 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``bold()`` surrounds a string in **boldness** modifiers for printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``plot_confusion_matrix()`` plots a single seaborn confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``plot_confusion_matrices()`` plots a series of six seaborn confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``summarize()`` prints a series of confusion matrices from (rounded) true and predicted y values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``macro_f1()`` just returns the macro F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading the validation data\n",
    "\n",
    "Here we load the validation data and flatten it or transform the time series data into MiniROCKET features, just like for the training data.\n",
    "\n",
    "Then, we concatenate the fixed-size data on soil quality etc. to the flattened and MiniROCKET features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f037bd724c46ed9c9855e34be3fdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 149685 samples\n",
      "test shape (149685, 14, 21)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e411e663064591bee079adb07b4408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664c17af016e4bf1a9b885f5e025e4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_static_valid, X_time_valid, y_target_valid = loadXY(dfs, \"test\",\n",
    "                                                     window_size=WINDOW_SIZE)\n",
    "print(\"test shape\", X_time_valid.shape)\n",
    "X_static_valid, X_time_valid = normer.normalize(X_static_valid, X_time_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create ``X_valid``, we are flattening out the time data, pretending there is no temporal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149685, 14, 21)\n",
      "(149685, 294)\n",
      "(149685, 30)\n",
      "(149685, 324)\n"
     ]
    }
   ],
   "source": [
    "print(X_time_valid.shape)\n",
    "\n",
    "X_valid = np.array(list(map(lambda x: x.flatten(), X_time_valid)))\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(X_static_valid.shape)\n",
    "\n",
    "X_valid = np.concatenate((X_valid, X_static_valid), axis=1)\n",
    "\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_test' + str(WINDOW_SIZE) + 'wvar', X_valid)\n",
    "np.save('y_test' + str(WINDOW_SIZE) + 'wvar', y_target_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save data to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data with pickles so we dont have to reload\n",
    "np.save('X_train_' + str(WINDOW_SIZE) + 'wvar', X_train)\n",
    "np.save('y_train_' + str(WINDOW_SIZE)+ 'wvar', y_target_train)\n",
    "np.save('X_valid_' + str(WINDOW_SIZE) + 'wvar', X_valid)\n",
    "np.save('y_valid_' + str(WINDOW_SIZE) + 'wvar', y_target_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate\n",
    "# from loader.py package use np_load\n",
    "#test1 = np_load('X_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(X_train.shape)\n",
    "#print(test1.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
